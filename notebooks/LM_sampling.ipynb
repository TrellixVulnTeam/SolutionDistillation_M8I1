{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys,os,os.path\n",
    "sys.path.append(os.path.expanduser('~/Codes/SolutionDistillation'))\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from distill.common.hparams import TransformerHparam, LSTMHparam\n",
    "from distill.data_util.prep_ptb import PTB\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from distill.common.hparams import TransformerHparam, LSTMHparam\n",
    "from distill.data_util.prep_ptb import PTB\n",
    "from distill.data_util.prep_sentwiki import SentWiki\n",
    "from distill.models.lm_lstm import LmLSTM\n",
    "import os\n",
    "from distill.pipelines.lm import LMTrainer\n",
    "\n",
    "# Enable TF Eager execution\n",
    "tfe = tf.contrib.eager\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "# Other setup\n",
    "Modes = tf.estimator.ModeKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string(\"exp_name\", \"trial\", \"\")\n",
    "tf.app.flags.DEFINE_string(\"task_name\", \"sent_wiki\", \"ptb_lm\")\n",
    "tf.app.flags.DEFINE_string(\"log_dir\", \"logs\", \"\")\n",
    "tf.app.flags.DEFINE_string(\"save_dir\", None, \"\")\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"model\", \"lm_lstm\", \"\")\n",
    "tf.app.flags.DEFINE_string(\"encoder_attention_dir\", \"top_down\", \"top_down | bottom_up\")\n",
    "tf.app.flags.DEFINE_integer(\"hidden_dim\", 728, \"\")\n",
    "tf.app.flags.DEFINE_integer(\"embedding_dim\",728, \"embeddings dim\")\n",
    "tf.app.flags.DEFINE_integer(\"output_dim\", 8000, \"\")\n",
    "tf.app.flags.DEFINE_integer(\"input_dim\", 8000, \"\")\n",
    "tf.app.flags.DEFINE_integer(\"number_of_heads\", 4, \"\")\n",
    "tf.app.flags.DEFINE_integer(\"ff_filter_size\", 512, \"\")\n",
    "tf.app.flags.DEFINE_float(\"initializer_gain\", 1.0, \"\")\n",
    "tf.app.flags.DEFINE_float(\"label_smoothing\", 0.1, \"\")\n",
    "tf.app.flags.DEFINE_boolean('train_embeddings', True, \" False | True\")\n",
    "tf.app.flags.DEFINE_string('sent_rep_mode', \"final\", \"none | final | all\")\n",
    "tf.app.flags.DEFINE_string('attention_mechanism',None, 'attention_mechanism')\n",
    "tf.app.flags.DEFINE_integer(\"depth\", 3, \"\")\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_float(\"input_dropout_keep_prob\", 1.0, \"\")\n",
    "tf.app.flags.DEFINE_float(\"hidden_dropout_keep_prob\", 1.0, \"\")\n",
    "\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.0001, \"\")\n",
    "tf.app.flags.DEFINE_boolean(\"decay_learning_rate\", True, \"True | False\")\n",
    "tf.app.flags.DEFINE_float(\"l2_rate\", 0.0001, \"\")\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 64, \"\")\n",
    "tf.app.flags.DEFINE_integer(\"training_iterations\", 300000, \"\")\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\"vocab_size\", 8000, \"\")\n",
    "tf.app.flags.DEFINE_boolean(\"bidirectional\", False, \"If the LSTM layer is bidirectional\")\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"pretrained_embedding_path\", \"data/sst/filtered_glove.txt\", \"pretrained embedding path\")\n",
    "tf.app.flags.DEFINE_string(\"data_path\", \"./data\", \"data path\")\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"f\",\"\",\"kernel\")\n",
    "hparams = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models = {\"lm_lstm\": LmLSTM}\n",
    "tasks = {'ptb_lm': PTB('../data/ptb'),\n",
    "       'sent_wiki': SentWiki('../data/sent_wiki')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task = tasks[hparams.task_name]\n",
    "hparams.vocab_size = tasks[hparams.task_name].vocab_length\n",
    "hparams.output_dim = len(tasks[hparams.task_name].target_vocab)\n",
    "lstm_params = LSTMHparam(input_dim=hparams.input_dim,\n",
    "                         hidden_dim=hparams.hidden_dim,\n",
    "                         output_dim=hparams.output_dim,\n",
    "                         encoder_depth=hparams.depth,\n",
    "                         decoder_depth=0,\n",
    "                         number_of_heads=hparams.number_of_heads,\n",
    "                         ff_filter_size=hparams.ff_filter_size,\n",
    "                       initializer_gain=hparams.initializer_gain,\n",
    "                       batch_size=hparams.batch_size,\n",
    "                       input_dropout_keep_prob=hparams.input_dropout_keep_prob,\n",
    "                       hidden_dropout_keep_prob=hparams.hidden_dropout_keep_prob,\n",
    "                       vocab_size=hparams.vocab_size,\n",
    "                       label_smoothing=hparams.label_smoothing,\n",
    "                       attention_mechanism=None,\n",
    "                       sent_rep_mode=hparams.sent_rep_mode,\n",
    "                       embedding_dim=hparams.embedding_dim,\n",
    "                       train_embeddings = hparams.train_embeddings,\n",
    "                       learning_rate=hparams.learning_rate)\n",
    "\n",
    "model_params = {\"lm_lstm\": lstm_params}\n",
    "model = Models[hparams.model](model_params[hparams.model],\n",
    "                                  task=tasks[hparams.task_name],\n",
    "                                  scope=hparams.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../logs/sent_wiki/lm_lstm_depth3_hidden_dim728_batch_size128_d3-5_run_38_ntied/model.ckpt-300000\n"
     ]
    }
   ],
   "source": [
    "save_dir = '../logs/sent_wiki/'+ \\\n",
    "'lm_lstm_depth3_hidden_dim728_batch_size128_d3-5_run_38_ntied'\n",
    "checkpoint = tf.train.latest_checkpoint(save_dir)\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/samira/Codes/SolutionDistillation/distill/layers/lstm.py:23: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9160ba1e80>: Note that this cell is not optimized for performance. Please use tf.contrib.cudnn_rnn.CudnnLSTM for better performance on GPU.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f90b158ecf8>: Note that this cell is not optimized for performance. Please use tf.contrib.cudnn_rnn.CudnnLSTM for better performance on GPU.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f90b1564828>: Note that this cell is not optimized for performance. Please use tf.contrib.cudnn_rnn.CudnnLSTM for better performance on GPU.\n",
      "WARNING:tensorflow:From /home/samira/Codes/SolutionDistillation/distill/layers/lstm.py:55: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "with tfe.restore_variables_on_create(checkpoint):\n",
    "    model.create_vars(reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('beta1_power', [])\n",
      "('beta2_power', [])\n",
      "('global_step', [])\n",
      "('lm_lstm/InputEmbed/embedding_and_softmax/weights', [9920, 728])\n",
      "('lm_lstm/InputEmbed/embedding_and_softmax/weights/AdamW', [9920, 728])\n",
      "('lm_lstm/InputEmbed/embedding_and_softmax/weights/AdamW_1', [9920, 728])\n",
      "('lm_lstm/InputEmbed/embedding_and_softmax/weights/ExponentialMovingAverage', [9920, 728])\n",
      "('lm_lstm/OutputEmbed/embedding_and_softmax/weights', [9920, 728])\n",
      "('lm_lstm/OutputEmbed/embedding_and_softmax/weights/AdamW', [9920, 728])\n",
      "('lm_lstm/OutputEmbed/embedding_and_softmax/weights/AdamW_1', [9920, 728])\n",
      "('lm_lstm/OutputEmbed/embedding_and_softmax/weights/ExponentialMovingAverage', [9920, 728])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_0/L0/bias', [2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_0/L0/bias/AdamW', [2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_0/L0/bias/AdamW_1', [2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_0/L0/bias/ExponentialMovingAverage', [2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_0/L0/kernel', [1456, 2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_0/L0/kernel/AdamW', [1456, 2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_0/L0/kernel/AdamW_1', [1456, 2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_0/L0/kernel/ExponentialMovingAverage', [1456, 2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_1/L2/bias', [2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_1/L2/bias/AdamW', [2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_1/L2/bias/AdamW_1', [2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_1/L2/bias/ExponentialMovingAverage', [2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_1/L2/kernel', [1456, 2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_1/L2/kernel/AdamW', [1456, 2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_1/L2/kernel/AdamW_1', [1456, 2912])\n",
      "('lm_lstm/lm_lstm/LSTM_Cells/rnn/multi_rnn_cell/cell_1/L2/kernel/ExponentialMovingAverage', [1456, 2912])\n",
      "('lm_lstm/output_bias', [9920])\n",
      "('lm_lstm/output_bias/AdamW', [9920])\n",
      "('lm_lstm/output_bias/AdamW_1', [9920])\n",
      "('lm_lstm/output_bias/ExponentialMovingAverage', [9920])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.framework.python.framework import checkpoint_utils\n",
    "var_list = checkpoint_utils.list_variables(checkpoint)\n",
    "for v in var_list:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "#with tfe.restore_variables_on_create(checkpoint):\n",
    "for i in np.arange(110):\n",
    "    with tfe.restore_variables_on_create(checkpoint):\n",
    "        with tf.variable_scope(model.scope, reuse=tf.AUTO_REUSE):\n",
    "            number_of_sample = 1000\n",
    "            initial_constant = task.word2id['<eos>']\n",
    "            \n",
    "            inputs = tf.ones((number_of_sample), dtype=tf.int32) * initial_constant\n",
    "            inputs_length = tf.map_fn(lambda  x: 1, inputs)\n",
    "            #samples = model.sample(sampling_initial_inputs, tf.map_fn(lambda  x: 1, sampling_initial_inputs))\n",
    "\n",
    "            def compute_decoding_step_input(current_input):\n",
    "                return None\n",
    "\n",
    "        def projection_layer(hidden_states):\n",
    "            return model.output_embedding_layer.linear(hidden_states) + model.output_bias\n",
    "    \n",
    "    lstm_decoder_output_dic = model.lstm.predict(inputs_length=inputs_length,\n",
    "                                                          target_length=100,\n",
    "                                                          compute_decoding_step_input_fn=compute_decoding_step_input,\n",
    "                                                          input_embedding_layer=model.input_embedding_layer,\n",
    "                                                          output_embedding_layer=projection_layer, eos_id=model.eos_id,\n",
    "                                                          is_train=False,\n",
    "                                                          initial_inputs=inputs)\n",
    "\n",
    "    predictions  = lstm_decoder_output_dic['samples'][:,:,0]\n",
    "    length = lstm_decoder_output_dic['outputs_lengths']\n",
    "    lengths = []\n",
    "    ind = 0\n",
    "    i = 0\n",
    "    for l,s in zip(list(length.numpy()),list(predictions.numpy())):\n",
    "        #print(s)\n",
    "        lengths.append(l)\n",
    "        for w in s:\n",
    "            if w not in task.id2word:\n",
    "                print(w)\n",
    "        sentences.append(' '.join(task.decode(s[:l], task.id2word)))\n",
    "        if l < length.numpy()[ind]:\n",
    "            ind = i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt', \"w\") as f:\n",
    "    f.writelines('\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tfe.restore_variables_on_create(checkpoint):\n",
    "    batch_size = 10\n",
    "    beam_width = 5\n",
    "    decoder_cell = model.lstm.multi_lstm_cell#tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    encoder_outputs = decoder_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(encoder_outputs, multiplier=beam_width)\n",
    "\n",
    "    my_decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell=decoder_cell,\n",
    "                                                      embedding=model.input_embedding_layer.shared_weights,\n",
    "                                                      start_tokens=tf.fill([batch_size],task.word2id['the']),\n",
    "                                                      end_token=task.word2id['<eos>'],\n",
    "                                                      initial_state=tiled_encoder_outputs,\n",
    "                                                      beam_width=beam_width)\n",
    "\n",
    "     # dynamic decoding\n",
    "    outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(my_decoder,\n",
    "                                                                       maximum_iterations=10,\n",
    "                                                                       output_time_major=True)\n",
    "    final_predicted_ids = outputs.predicted_ids\n",
    "    scores = outputs.beam_search_decoder_output.scores.numpy().transpose()\n",
    "    predicted_ids = outputs.beam_search_decoder_output.predicted_ids\n",
    "    parent_ids = outputs.beam_search_decoder_output.parent_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['far', 'points', 'points', 'car', 'car', 'car', 'parts', 'parts', 'parts', 'parts']\n",
      "['far', 'points', 'points', 'car', 'car', 'car', 'parts', 'parts', 'parts', 'parts']\n",
      "['far', 'points', 'points', 'car', 'car', 'car', 'parts', 'parts', 'parts', 'parts']\n",
      "['far', 'points', 'points', 'car', 'car', 'car', 'parts', 'parts', 'parts', 'parts']\n",
      "['far', 'points', 'points', 'car', 'car', 'car', 'parts', 'parts', 'parts', 'parts']\n",
      "['far', 'points', 'points', 'car', 'car', 'car', 'parts', 'parts', 'parts', 'parts']\n",
      "['far', 'points', 'points', 'car', 'car', 'car', 'parts', 'parts', 'parts', 'parts']\n",
      "['far', 'points', 'points', 'car', 'car', 'car', 'parts', 'parts', 'parts', 'parts']\n",
      "['far', 'points', 'points', 'car', 'car', 'car', 'parts', 'parts', 'parts', 'parts']\n",
      "['far', 'points', 'points', 'car', 'car', 'car', 'parts', 'parts', 'parts', 'parts']\n"
     ]
    }
   ],
   "source": [
    "predicted_ids = outputs.beam_search_decoder_output.predicted_ids\n",
    "for s in predicted_ids.numpy().transpose()[np.argmax(np.sum(scores, axis=-1))]:\n",
    "    print(task.decode(s, task.id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tfe.restore_variables_on_create(checkpoint):\n",
    "    embeddings = model.input_embedding_layer.shared_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding_layer.shared_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(task.get_tfrecord_path(mode=\"train\"))\n",
    "dataset = dataset.map(task.parse_examples)\n",
    "dataset = dataset.padded_batch(hparams.batch_size, padded_shapes=task.get_padded_shapes())\n",
    "dataset = dataset.shuffle(buffer_size=1000)\n",
    "dataset = dataset.repeat()\n",
    "train_iterator = dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = train_iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in examples[0].numpy():\n",
    "    print(task.decode(e, task.id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
