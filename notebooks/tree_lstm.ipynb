{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, time, itertools\n",
    "import math, random\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Vocab(object):\n",
    "  def __init__(self):\n",
    "    self.word_to_index = {}\n",
    "    self.index_to_word = {}\n",
    "    self.unknown = '<unk>'\n",
    "    self.add_word(self.unknown, count=0)\n",
    "\n",
    "  def add_word(self, word, count=1):\n",
    "    if word not in self.word_to_index:\n",
    "      index = len(self.word_to_index)\n",
    "      self.word_to_index[word] = index\n",
    "      self.index_to_word[index] = word\n",
    "\n",
    "  def build_vocab(self, words):\n",
    "    for word in words:\n",
    "      self.add_word(word)\n",
    "    print('{} total unique words'.format(len(self.word_to_index)))\n",
    "\n",
    "  def encode(self, tokens):\n",
    "    if type(tokens) is str:\n",
    "      tokens = [tokens]\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "      if token not in self.word_to_index:\n",
    "        token = self.unknown\n",
    "      ids.append(self.word_to_index[token])\n",
    "\n",
    "    return ids\n",
    "\n",
    "  def decode(self, ids):\n",
    "    if type(ids) is int:\n",
    "      ids = [id]\n",
    "    tokens = []\n",
    "    for id in ids:\n",
    "     tokens.append(self.index_to_word[id])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.word_freq)\n",
    "\n",
    "  def save(self,vocab_file_name):\n",
    "    save_dic = {\n",
    "        'word_to_index': self.word_to_index,\n",
    "        'index_to_word':self.index_to_word,\n",
    "    }\n",
    "    np.save(vocab_file_name, save_dic)\n",
    "    \n",
    "  def load(self,vocab_file_name):\n",
    "    loaded_dic = np.load(vocab_file_name+\".npy\").item()\n",
    "    self.word_to_index = loaded_dic['word_to_index']\n",
    "    self.index_to_word = loaded_dic['index_to_word']\n",
    "    \n",
    "class Node:  # a node in the tree\n",
    "  def __init__(self, label, word=None):\n",
    "    self.label = label\n",
    "    self.word = word\n",
    "    self.parent = None  # reference to parent\n",
    "    self.left = None  # reference to left child\n",
    "    self.right = None  # reference to right child\n",
    "    # true if I am a leaf (could have probably derived this from if I have\n",
    "    # a word)\n",
    "    self.isLeaf = False\n",
    "    # true if we have finished performing fowardprop on this node (note,\n",
    "    # there are many ways to implement the recursion.. some might not\n",
    "    # require this flag)\n",
    "\n",
    "  def __str__(self):\n",
    "    if self.isLeaf:\n",
    "      return '[{0}:{1}]'.format(self.word, self.label)\n",
    "    return '({0} <- [{1}:{2}] -> {3})'.format(self.left, self.word, self.label, self.right)\n",
    "\n",
    "\n",
    "class Tree:\n",
    "\n",
    "  def __init__(self, treeString, openChar='(', closeChar=')'):\n",
    "    tokens = []\n",
    "    self.open = '('\n",
    "    self.close = ')'\n",
    "    for toks in treeString.strip().split():\n",
    "      tokens += list(toks)\n",
    "    self.root = self.parse(tokens)\n",
    "    # get list of labels as obtained through a post-order traversal\n",
    "    self.labels = get_labels(self.root)\n",
    "    self.num_words = len(self.labels)\n",
    "\n",
    "  def parse(self, tokens, parent=None):\n",
    "    assert tokens[0] == self.open, \"Malformed tree\"\n",
    "    assert tokens[-1] == self.close, \"Malformed tree\"\n",
    "\n",
    "    split = 2  # position after open and label\n",
    "    countOpen = countClose = 0\n",
    "\n",
    "    if tokens[split] == self.open:\n",
    "      countOpen += 1\n",
    "      split += 1\n",
    "    # Find where left child and right child split\n",
    "    while countOpen != countClose:\n",
    "      if tokens[split] == self.open:\n",
    "        countOpen += 1\n",
    "      if tokens[split] == self.close:\n",
    "        countClose += 1\n",
    "      split += 1\n",
    "\n",
    "    # New node\n",
    "    node = Node(int(tokens[1]))  # zero index labels\n",
    "\n",
    "    node.parent = parent\n",
    "\n",
    "    # leaf Node\n",
    "    if countOpen == 0:\n",
    "      node.word = ''.join(tokens[2:-1]).lower()  # lower case?\n",
    "      node.isLeaf = True\n",
    "      return node\n",
    "\n",
    "    node.left = self.parse(tokens[2:split], parent=node)\n",
    "    node.right = self.parse(tokens[split:-1], parent=node)\n",
    "\n",
    "    return node\n",
    "\n",
    "  def get_words(self):\n",
    "    leaves = getLeaves(self.root)\n",
    "    words = [node.word for node in leaves]\n",
    "    return words\n",
    "\n",
    "\n",
    "def leftTraverse(node, nodeFn=None, args=None):\n",
    "  \"\"\"\n",
    "  Recursive function traverses tree\n",
    "  from left to right.\n",
    "  Calls nodeFn at each node\n",
    "  \"\"\"\n",
    "  if node is None:\n",
    "    return\n",
    "  leftTraverse(node.left, nodeFn, args)\n",
    "  leftTraverse(node.right, nodeFn, args)\n",
    "  nodeFn(node, args)\n",
    "\n",
    "\n",
    "def getLeaves(node):\n",
    "  if node is None:\n",
    "    return []\n",
    "  if node.isLeaf:\n",
    "    return [node]\n",
    "  else:\n",
    "    return getLeaves(node.left) + getLeaves(node.right)\n",
    "\n",
    "\n",
    "def get_labels(node):\n",
    "  if node is None:\n",
    "    return []\n",
    "  return get_labels(node.left) + get_labels(node.right) + [node.label]\n",
    "\n",
    "\n",
    "def clearFprop(node, words):\n",
    "  node.fprop = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train trees..\n"
     ]
    }
   ],
   "source": [
    "dataSet = \"train\"\n",
    "file = 'trees/%s.txt' % dataSet\n",
    "print(\"Loading %s trees..\" % dataSet)\n",
    "with open(file, 'r') as fid:\n",
    "    trees = [Tree(l) for l in fid.readlines()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16581 total unique words\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab()\n",
    "# Get list of tokenized sentences\n",
    "train_sents = [t.get_words() for t in trees]\n",
    "# Get list of all words\n",
    "all_words = list(itertools.chain.from_iterable(train_sents))\n",
    "\n",
    "# Build Vocab\n",
    "vocab.build_vocab(all_words)\n",
    "vocab.save('sst_vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.load('sst_vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_features = []\n",
    "for example_id, tree in enumerate(trees):\n",
    "    words = tree.get_words()\n",
    "    node  = tree.root\n",
    "    nodes_list = []\n",
    "    leftTraverse(node, lambda node, args: args.append(node), nodes_list)\n",
    "    node_to_index = OrderedDict()\n",
    "    for i in range(len(nodes_list)):\n",
    "      node_to_index[nodes_list[i]] = i\n",
    "    example_features.append({\n",
    "      'example_id': example_id,\n",
    "      'is_leaf': [int(node.isLeaf) for node in nodes_list],\n",
    "      'left_children': [node_to_index[node.left] if\n",
    "                                         not node.isLeaf else -1\n",
    "                                         for node in nodes_list],\n",
    "      'right_children': [node_to_index[node.right] if\n",
    "                                          not node.isLeaf else -1\n",
    "                                          for node in nodes_list],\n",
    "      'node_word_ids': [vocab.encode(node.word)[0] if\n",
    "                                             node.word else -1\n",
    "                                             for node in nodes_list],\n",
    "      'labels': [node.label for node in nodes_list],\n",
    "      'length': len(nodes_list)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_features(example_feaures):\n",
    "    \"\"\"Convert our own representation of an example's features to Features class for TensorFlow dataset.\n",
    "    \"\"\"\n",
    "    features = tf.train.Features(feature={\n",
    "        \"example_id\": tf.train.Feature(int64_list=tf.train.Int64List(value=[example_feaures['example_id']])),\n",
    "        \"is_leaf\": tf.train.Feature(int64_list=tf.train.Int64List(value=example_feaures['is_leaf'])),\n",
    "        \"left_children\": tf.train.Feature(int64_list=tf.train.Int64List(value=example_feaures['left_children'])),\n",
    "        \"right_children\": tf.train.Feature(int64_list=tf.train.Int64List(value=example_feaures['right_children'])),\n",
    "        \"node_word_ids\": tf.train.Feature(int64_list=tf.train.Int64List(value=example_feaures['node_word_ids'])),\n",
    "        \"labels\": tf.train.Feature(int64_list=tf.train.Int64List(value=example_feaures['labels'])),\n",
    "        \"length\": tf.train.Feature(int64_list=tf.train.Int64List(value=[example_feaures['length']])),\n",
    "      })\n",
    "    return features\n",
    "\n",
    "tf_example_features = []\n",
    "for example in example_features:\n",
    "       tf_example_features.append(get_tf_features(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8544/8544 [00:00<00:00, 9859.53it/s] \n"
     ]
    }
   ],
   "source": [
    "with tf.python_io.TFRecordWriter('train_trees') as tf_record_writer:\n",
    "    for example in tqdm(tf_example_features):\n",
    "        tf_record = tf.train.Example(features=example)\n",
    "        tf_record_writer.write(tf_record.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor(\"ParseSingleExample/ParseSingleExample:4\", shape=(), dtype=int64)\n",
      "INFO:tensorflow:Tensor(\"ParseSingleExample/ParseSingleExample:1\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def parse_sst_tree_examples(example):\n",
    "    \"\"\"Load an example from TF record format.\"\"\"\n",
    "    features = {\"example_id\": tf.FixedLenFeature([], tf.int64),\n",
    "                \"length\": tf.FixedLenFeature([], tf.int64),\n",
    "               \"is_leaf\": tf.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\n",
    "               \"left_children\": tf.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\n",
    "               \"right_children\": tf.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\n",
    "               \"node_word_ids\": tf.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\n",
    "               \"labels\": tf.FixedLenSequenceFeature([], tf.int64, allow_missing=True)}\n",
    "    parsed_example = tf.parse_single_example(example, features=features)\n",
    "    \n",
    "    example_id = parsed_example[\"example_id\"]\n",
    "    length = parsed_example[\"length\"]\n",
    "    tf.logging.info(length)\n",
    "    is_leaf = parsed_example[\"is_leaf\"]\n",
    "    tf.logging.info(is_leaf)\n",
    "    left_children = parsed_example[\"left_children\"]\n",
    "    right_children = parsed_example[\"right_children\"]\n",
    "    node_word_ids = parsed_example[\"node_word_ids\"]\n",
    "    labels = parsed_example[\"labels\"]\n",
    "\n",
    "    return example_id, length, is_leaf, left_children, right_children, node_word_ids, labels\n",
    "                \n",
    "dataset = tf.data.TFRecordDataset(\"train_trees\")\n",
    "dataset = dataset.map(parse_sst_tree_examples)\n",
    "dataset = dataset.padded_batch(10, padded_shapes=([],[],[None],[None],[None],[None],[None]))\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "example_id, length, is_leaf, left_children, right_children, node_word_ids, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1\n",
      "  1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1\n",
      "  0 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0]\n",
      " [1 1 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0\n",
      "  0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0\n",
      "  0 0 0 1 0]\n",
      " [1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0\n",
      "  0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [1 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(iterator.initializer)\n",
    "    print(sess.run(is_leaf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
