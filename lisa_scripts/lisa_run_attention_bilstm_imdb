#!/bin/bash

#SBATCH --job-name=bilstm_imdb
#SBATCH --ntasks=2
#SBATCH --cpus-per-task=1
#SBATCH --ntasks-per-node=1
#SBATCH --time=5:00:00
#SBATCH --partition=gpu_shared
#SBATCH --gres=gpu:2
#SBATCH --mem=60G

module load eb
module load Python/3.6.3-foss-2017b
module load CUDA/10.0.130
module load cuDNN/7.3.1-CUDA-10.0.130
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:/hpc/eb/Debian/cuDNN/7.3.1-CUDA-10.0.130/lib64:$LD_LIBRARY_PATH

cd /home/samigpu/Codes/SolutionDistillation

HEAD_DIR="/home/samigpu/Codes/SolutionDistillation"
CODE_DIR=$HEAD_DIR/distill
DATA_DIR=$HEAD_DIR/data
LOGS_DIR=$HEAD_DIR/logs

export PYTHONPATH=$PYTHONPATH:$HEAD_DIR

python $CODE_DIR/seq2seq_trainer.py --model=bilstm --batch_size=16 --hidden_dim=300  \
--train_embeddings=False --decoder_depth=1 --encoder_depth=2 \
--input_dropout_keep_prob=0.8 --hidden_dropout_keep_prob=0.1 --learning_rate=0.001 \
--decay_learning_rate=True --l2_rate=0.001 \
--task_name=imdb --exp_name=_run1 &


python $CODE_DIR/seq2seq_trainer.py --model=bilstm --batch_size=16 --hidden_dim=300  \
--train_embeddings=True --decoder_depth=1 --encoder_depth=2 \
--input_dropout_keep_prob=0.8 --hidden_dropout_keep_prob=0.1 --learning_rate=0.001 \
--decay_learning_rate=True --l2_rate=0.001 \
--task_name=imdb --exp_name=_trainembs_run1